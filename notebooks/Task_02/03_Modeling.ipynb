{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcWLTLp1JQKQ/du0cMcyrZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yenuli0808/CM2604_Telco_Customer_Churn-CW/blob/main/notebooks/Task_02/03_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 03: Modeling Decision Tress and Neural Network Models**"
      ],
      "metadata": {
        "id": "I9skWZP0DDKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.a: Import Libraries**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ATOtqzUXTBFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os, joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
        "                             roc_auc_score, roc_curve, precision_recall_curve, auc, brier_score_loss)\n",
        "from sklearn.inspection import permutation_importance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "print(\"Versions Being Used :\\n\")\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"pandas:\", pd.__version__, \"numpy:\", np.__version__)\n",
        "import sklearn\n",
        "print(\"sklearn:\", sklearn.__version__)\n",
        "import imblearn\n",
        "print(\"imblearn:\", imblearn.__version__)\n",
        "import tensorflow as tf\n",
        "print(\"tensorflow:\", tf.__version__)\n",
        "\n",
        "print(\"\\n✅ Libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "fpn5iyTNbozE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.b: Import Cleaned Data Set**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PbN57X2PDt4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cleaned data set from the git\n",
        "url_clean = \"https://raw.githubusercontent.com/Yenuli0808/CM2604_Telco_Customer_Churn-CW/main/data/Cleaned_Telco_Customer_Churn.csv\"\n",
        "\n",
        "df = pd.read_csv(url_clean)\n",
        "\n",
        "print(\"✅ Cleaned Dataset loaded successfully!\")\n",
        "print(\"\\n=== Cleaned Data set overview ===\")\n",
        "print(f\"Dataset shape: {df.shape}\")"
      ],
      "metadata": {
        "id": "XTghYE1iF70U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.c: Quick Look-up on Dataset**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "suNHMfFUUg5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First look at the cleaned dataset\n",
        "print(\"==== FIRST 10 ROWS ====\\n\")\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "Fiq_N8GDGrnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClass distribution:\\n\")\n",
        "class_dist = pd.DataFrame({\n",
        "    \"Count\": df[\"Churn\"].value_counts(),\n",
        "    \"Percentage (%)\": (df[\"Churn\"].value_counts(normalize=True) * 100).round(2)\n",
        "})\n",
        "class_dist"
      ],
      "metadata": {
        "id": "Kh4V-gricv7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.d: Train-Test Split (stratified) and Variable Lists**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cZi5E4lCZWOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining features and target\n",
        "X = df.drop(\"Churn\", axis=1)\n",
        "y = df[\"Churn\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# Class distribution in training set\n",
        "churn_table_train = pd.DataFrame({\n",
        "    'Count': y_train.value_counts(),\n",
        "    'Percentage (%)': (y_train.value_counts(normalize=True) * 100).round(2)\n",
        "})\n",
        "\n",
        "# Class distribution in testing set\n",
        "churn_table_test = pd.DataFrame({\n",
        "    'Count': y_test.value_counts(),\n",
        "    'Percentage (%)': (y_test.value_counts(normalize=True) * 100).round(2)\n",
        "})\n",
        "\n",
        "print(\"Training set: \", X_train.shape)\n",
        "print(\"\\nClass distribution in training set: \")\n",
        "print(churn_table_train)\n",
        "\n",
        "print(\"\\nTesting set: \", X_test.shape)\n",
        "print(\"\\nClass distribution in testing set: \")\n",
        "print(churn_table_test)\n",
        "\n",
        "# Combine the tables for a comprehensive view\n",
        "combined_churn_distribution = pd.concat({\n",
        "    'Train': churn_table_train,\n",
        "    'Test': churn_table_test\n",
        "}, axis=1)\n",
        "\n",
        "print(\"\\nCombined Class Distribution (Train vs. Test):\\n\")\n",
        "print(combined_churn_distribution)"
      ],
      "metadata": {
        "id": "CoSK6UWkdzyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying categorical and numerical columns\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "print(\"Categorical columns:\", categorical_cols,\"\\n\")\n",
        "print(\"Numerical columns:\", numerical_cols)"
      ],
      "metadata": {
        "id": "_vdaC0NieVz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.e: Preprocessing Pipeline**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "u8fvYrqCEGGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(),numerical_cols),\n",
        "    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'),categorical_cols)\n",
        "])\n",
        "\n",
        "print(\"✅ Preprocessing pipeline created successfully!\")\n",
        "print(\"\\n=== Preprocessing pipeline overview ===\")\n",
        "preprocessor"
      ],
      "metadata": {
        "id": "tn9oax-Nf9TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit transform train, transform test\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "print(\"✅ Preprocessing completed successfully!\")\n",
        "print(\"\\n=== Preprocessed Data set overview ===\")\n",
        "print(f\"\\nTraining set shape: {X_train_preprocessed.shape}\")\n",
        "print(f\"Testing set shape: {X_test_preprocessed.shape}\")"
      ],
      "metadata": {
        "id": "c1S_l1I2gimS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.f: Helper Evaluate Functions**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QZseV4SaiNr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_eval_full(model_name,\n",
        "                    y_train, y_train_pred, y_train_proba,\n",
        "                    y_test,  y_test_pred,  y_test_proba):\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(f\"MODEL EVALUATION: {model_name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # -------------------------\n",
        "    # TRAINING METRICS\n",
        "    # -------------------------\n",
        "    print(\"\\n TRAINING PERFORMANCE\")\n",
        "    print(\"Accuracy:\", round(accuracy_score(y_train, y_train_pred)*100, 4))\n",
        "    print(\"ROC AUC:\", round(roc_auc_score(y_train, y_train_proba), 4))\n",
        "    print(\"\\nClassification Report (Train):\")\n",
        "    print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm_train, annot=True, fmt='d', cmap='Reds')\n",
        "    plt.title(f\"Confusion Matrix - TRAIN ({model_name})\")\n",
        "    plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "\n",
        "    # -------------------------\n",
        "    # TESTING METRICS\n",
        "    # -------------------------\n",
        "    print(\"\\n TESTING PERFORMANCE\")\n",
        "    print(\"Accuracy:\", round(accuracy_score(y_test, y_test_pred)*100, 4))\n",
        "    print(\"ROC AUC:\", round(roc_auc_score(y_test, y_test_proba), 4))\n",
        "    print(\"\\nClassification Report (Test):\")\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f\"Confusion Matrix - TEST ({model_name})\")\n",
        "    plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"=\"*60)\n"
      ],
      "metadata": {
        "id": "Ia8Sy0MviYGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.1: Decision Tree Model**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "A5j14Z6OO9C4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1.1: Hyperparameter tuning for Decision Tress**"
      ],
      "metadata": {
        "id": "Dyq7ILlFsZhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depths = [3,5,7,10,None]\n",
        "min_sample_split = [2,5,10]\n",
        "min_sample_leaf = [1,2,4]\n",
        "criterion = ['gini', 'entropy']\n",
        "\n",
        "best_dt_auc = -1\n",
        "best_dt_params = {}\n",
        "best_model_dt = None\n",
        "results = []\n",
        "\n",
        "for d in depths:\n",
        "  for ms in min_sample_split:\n",
        "    for ml in min_sample_leaf:\n",
        "      for crit in criterion:\n",
        "        dt = DecisionTreeClassifier(\n",
        "          max_depth=d,\n",
        "          min_samples_split=ms,\n",
        "          min_samples_leaf=ml,\n",
        "          criterion=crit,\n",
        "          random_state=RANDOM_STATE\n",
        "      )\n",
        "      dt.fit(X_train_preprocessed, y_train)\n",
        "      proba = dt.predict_proba(X_test_preprocessed)[:, 1]\n",
        "      auc_score = roc_auc_score(y_test, proba)\n",
        "      results.append((auc_score, d, ms, ml))\n",
        "\n",
        "      if auc_score > best_dt_auc:\n",
        "        best_dt_auc = auc_score\n",
        "        best_model_dt = dt\n",
        "        best_dt_params = {'depths':d, 'min_sample_split':ms, 'min_sample_leaf':ml, \"criterion\":crit }\n",
        "\n",
        "\n",
        "print(\"Best DT Parameters:\", best_dt_params)\n",
        "print(\"\\nBest DT AUC Score:\", best_dt_auc)\n",
        "\n",
        "#Showing top 5 configs by AUC\n",
        "results_sorted = sorted(results, reverse=True, key=lambda x: x[0])[:5]\n",
        "print(\"\\nTop 5 DT Configurations:\")\n",
        "for r in results_sorted:\n",
        "  print(f\"AUC: {r[0]}, Depth: {r[1]}, Min Sample Split: {r[2]}, Min Sample Leaf: {r[3]}\")\n",
        "\n",
        "# Croo validation on train for best config (stability)\n",
        "cv_scores = cross_val_score(best_model_dt, X_train_preprocessed, y_train, cv=5, scoring='roc_auc', n_jobs=-1)\n",
        "print(\"\\nCV AUC (train) for best DT: %.4f ± %.4f\" % (cv_scores.mean(), cv_scores.std()))"
      ],
      "metadata": {
        "id": "SHv3baZgCDPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1.2: Decision Tree Evaluation**"
      ],
      "metadata": {
        "id": "SqVk-FzPK2Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba_dt = best_model_dt.predict_proba(X_test_preprocessed)[:,1]\n",
        "y_pred_dt = (y_proba_dt > 0.5).astype(int)\n",
        "\n",
        "y_pred_proba_train_dt = best_model_dt.predict_proba(X_train_preprocessed)[:,1]\n",
        "y_pred_train_dt = (y_pred_proba_train_dt > 0.5).astype(int)\n",
        "print_eval_full(\"Decision Tree  - Test\",\n",
        "                y_train, y_pred_train_dt, y_pred_proba_train_dt,\n",
        "                y_test, y_pred_dt, y_proba_dt)"
      ],
      "metadata": {
        "id": "cDIqWNYr3pEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1.2.1: ROC Curve**"
      ],
      "metadata": {
        "id": "G4bE0U-5MOa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr_dt, tpr_dt, _ =roc_curve(y_test, y_pred_proba_dt)\n",
        "plt.figure(figsize=(7,6))\n",
        "plt.plot(fpr_dt, tpr_dt, label=f'ROC Curve (AUC = {roc_auc_score(y_test, y_pred_proba_dt):.3f})')\n",
        "plt.plot([0,1], [0,1], 'k--')\n",
        "plt.title('Decision Tree - ROC Curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ffUN3neddhkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1.2.2: DT-Precision Recall Curve**"
      ],
      "metadata": {
        "id": "rpsPctRfOQdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba_dt = best_model_dt.predict_proba(X_test_preprocessed)[:, 1]\n",
        "\n",
        "precision_dt, recall_dt, thresholds_dt = precision_recall_curve(y_test, y_pred_proba_dt)\n",
        "pr_auc_dt = auc(recall_dt, precision_dt)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(recall_dt, precision_dt, label=f'Decision Tree (PR-AUC = {pr_auc_dt:.3f})', color='teal')\n",
        "plt.xlabel('Recall (Sensitivity) - How many actual churners did we catch?')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Decision Tree')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c6GZMFmKd-Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1.3: DT- Feature Imporatance**"
      ],
      "metadata": {
        "id": "6Ooy0KK8O-4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols).tolist()\n",
        "feature_names = numerical_cols + encoded_feature_names\n",
        "\n",
        "importance = best_model_dt.feature_importances_\n",
        "feat_imp = pd.Series(importance, index=feature_names).sort_values(ascending=False)[:15]\n",
        "\n",
        "plt.figure(figsize=(15,11))\n",
        "feat_imp.plot(kind='barh')\n",
        "plt.title('Top 15 Feature Importance - Decision Tree')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s1Lt7JWCvYr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1.4: BaseLine Decision Tree (No-Tuning)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "H-Ph8xKOwY18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
        "baseline_dt.fit(X_train_preprocessed, y_train)\n",
        "\n",
        "y_proba_base_dt = baseline_dt.predict_proba(X_test_preprocessed)[:,1]\n",
        "y_pred_base_dt = (y_proba_base_dt > 0.5).astype(int)\n",
        "\n",
        "print_eval_full(\"Baseline Decision Tree (No Tuning)\",\n",
        "           y_train, y_pred_train_dt, y_pred_proba_train_dt,\n",
        "           y_test, y_pred_base_dt, y_proba_base_dt)\n"
      ],
      "metadata": {
        "id": "DegiRGPYwnWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.2: Neural Network Model (Keras)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "prjeAfKPU9p3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2.1: Hyperparameter tuning for NN**"
      ],
      "metadata": {
        "id": "X-j2Zuqvs9NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "layer_configs = [[64, 32], [128, 64], [128, 64, 32]]\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "dropouts = [0.0, 0.2, 0.3]\n",
        "epochs = 30\n",
        "batch_size = 32\n",
        "\n",
        "best_nn_acc = -1\n",
        "best_nn = None\n",
        "best_nn_params = {}\n",
        "best_history = None\n",
        "t0 = time.time()\n",
        "\n",
        "for layers in layer_configs:\n",
        "    for lr in learning_rates:\n",
        "        for drop in dropouts:\n",
        "            tf.keras.backend.clear_session()\n",
        "            model = Sequential()\n",
        "            model.add(Dense(layers[0], activation='relu', input_shape=(X_train_preprocessed.shape[1],)))\n",
        "            if drop > 0:\n",
        "                model.add(Dropout(drop))\n",
        "            for units in layers[1:]:\n",
        "                model.add(Dense(units, activation='relu'))\n",
        "                if drop > 0:\n",
        "                    model.add(Dropout(drop))\n",
        "            model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "            model.compile(optimizer=Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "            history = model.fit(X_train_preprocessed, y_train, validation_split=0.2,\n",
        "                             epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "            test_loss, test_acc = model.evaluate(X_test_preprocessed, y_test, verbose=0) # Added verbose=0 to suppress output during tuning\n",
        "            if test_acc > best_nn_acc:\n",
        "                best_nn_acc = test_acc\n",
        "                best_nn = model\n",
        "                best_nn_params = {'layers': layers, 'lr': lr, 'dropout': drop}\n",
        "                best_history = history\n",
        "\n",
        "print(\"Best NN Parameters (manual tuning):\", best_nn_params)\n",
        "print(\"Best NN Testing Accuracy:\", best_nn_acc)\n",
        "print(\"Time elapsed (s):\", round(time.time()-t0,1))"
      ],
      "metadata": {
        "id": "mvBcPj43nxwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2.2: NN Evaluation**"
      ],
      "metadata": {
        "id": "zWBAGkYC5Nln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba_nn = best_nn.predict(X_test_preprocessed).ravel()\n",
        "y_pred_nn = (y_proba_nn > 0.5).astype(int)\n",
        "\n",
        "y_proba_train_nn = best_nn.predict(X_train_preprocessed).ravel()\n",
        "y_pred_train_nn = (y_proba_train_nn > 0.5).astype(int)\n",
        "\n",
        "print_eval_full(\"Neural Network(Tuning)\",\n",
        "                y_train, y_pred_train_nn, y_proba_train_nn,\n",
        "                y_test, y_pred_nn, y_proba_nn)"
      ],
      "metadata": {
        "id": "VSE3cEUV5R76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2.2.1: ROC Curve**"
      ],
      "metadata": {
        "id": "tfsclBaiOCbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr_nn, tpr_nn, _ = roc_curve(y_test, y_proba_nn.flatten())\n",
        "plt.figure(figsize=(7,6))\n",
        "plt.plot(fpr_nn, tpr_nn, label=f'ROC (AUC = {roc_auc_score(y_test, y_proba_nn.flatten()):.3f})')\n",
        "plt.plot([0,1], [0,1], 'k--')\n",
        "plt.title('ROC Curve - Neural Network')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mUvhsZyzOJwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2.2.2: NN-Precision Recall Curve**"
      ],
      "metadata": {
        "id": "i8DR3a2VOhi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision_nn, recall_nn, _ = precision_recall_curve(y_test, y_proba_nn.flatten())\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(recall_nn, precision_nn, label=f'PR (AUC = {auc(recall_nn, precision_nn):.3f})')\n",
        "plt.title('Precision-Recall Curve - Neural Network')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kgArKtXYOlc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2.2.3: Training Curves for NN**"
      ],
      "metadata": {
        "id": "-90UNSRoOuMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(best_history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(best_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('NN Training Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Acuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(best_history.history['loss'], label='Training Loss')\n",
        "plt.plot(best_history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('NN Training Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-P75_DyV86Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.3:Synthetic Minority Oversampling Technique (SMOTE)**"
      ],
      "metadata": {
        "id": "LBFXqGJO_yPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only apply for the training dataset."
      ],
      "metadata": {
        "id": "cTkqn-MgFmfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm = SMOTE(random_state=RANDOM_STATE)\n",
        "X_train_sm, y_train_sm = sm.fit_resample(X_train_preprocessed, y_train)\n",
        "\n",
        "print(\"Before SMOTE:\", np.bincount(y_train))\n",
        "print(\"After SMOTE:\", np.bincount(y_train_sm))"
      ],
      "metadata": {
        "id": "zEysRbuDBS8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3.1: Decision Tree tuning ( with SMOTE)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1RIeba9VB0aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_auc_sm = -1\n",
        "best_dt_sm = None\n",
        "best_params_sm = None\n",
        "results_sm = []\n",
        "\n",
        "for d in depths:\n",
        "    for ms in min_sample_split:\n",
        "        for ml in min_sample_leaf:\n",
        "            dt = DecisionTreeClassifier(max_depth=d, min_samples_split=ms,\n",
        "                                        min_samples_leaf=ml, random_state=RANDOM_STATE)\n",
        "            dt.fit(X_train_sm, y_train_sm)\n",
        "            proba = dt.predict_proba(X_test_preprocessed)[:,1]\n",
        "            auc_score = roc_auc_score(y_test, proba)\n",
        "            results_sm.append((auc_score, d, ms, ml))\n",
        "            if auc_score > best_auc_sm:\n",
        "                best_auc_sm = auc_score\n",
        "                best_dt_sm = dt\n",
        "                best_params_sm = (d, ms, ml)\n",
        "\n",
        "print(\"Best DT (SMOTE) params:\", best_params_sm, \"AUC:\", best_auc_sm)\n",
        "cv_scores_sm = cross_val_score(best_dt_sm, X_train_sm, y_train_sm, cv=5, scoring='roc_auc', n_jobs=-1)\n",
        "print(\"CV AUC (SMOTE train) for best DT:\", cv_scores_sm.mean(), cv_scores_sm.std())"
      ],
      "metadata": {
        "id": "7kiEMBo0CbBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3.1.1: Evaluate DT (With SMOTE)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QnfelDE-DHOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba_dt_sm = best_dt_sm.predict_proba(X_test_preprocessed)[:,1]\n",
        "y_pred_dt_sm = (y_proba_dt_sm > 0.5).astype(int)\n",
        "\n",
        "y_pred_proba_train_dt_sm = best_dt_sm.predict_proba(X_train_sm)[:,1]\n",
        "y_pred_train_dt_sm = (y_pred_proba_train_dt_sm > 0.5).astype(int)\n",
        "\n",
        "print_eval_full(\"Decision Tree (with SMOTE)\",\n",
        "                y_train_sm, y_pred_train_dt_sm, y_pred_proba_train_dt_sm,\n",
        "                y_test, y_pred_dt_sm, y_proba_dt_sm)"
      ],
      "metadata": {
        "id": "oO1I4lNTDWG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3.2: Neural Network Training (With SMOTE)**"
      ],
      "metadata": {
        "id": "HqnP7iIdFSUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tune (small grid) on SMOTE data\n",
        "best_nn_acc_sm = -1\n",
        "best_nn_sm = None\n",
        "best_hist_sm = None\n",
        "best_params_nn_sm = None\n",
        "t0 = time.time()\n",
        "\n",
        "for layers in layer_configs:\n",
        "    for lr in learning_rates:\n",
        "        for dr in dropouts:\n",
        "            tf.keras.backend.clear_session()\n",
        "            model = Sequential()\n",
        "            model.add(Dense(layers[0], activation='relu', input_shape=(X_train_sm.shape[1],)))\n",
        "            if dr > 0:\n",
        "                model.add(Dropout(dr))\n",
        "            for units in layers[1:]:\n",
        "                model.add(Dense(units, activation='relu'))\n",
        "                if dr > 0:\n",
        "                    model.add(Dropout(dr))\n",
        "            model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "            model.compile(optimizer=Adam(learning_rate=lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "            hist = model.fit(X_train_sm, y_train_sm, validation_split=0.2,\n",
        "                             epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "            test_loss, test_acc = model.evaluate(X_test_preprocessed, y_test, verbose=0)\n",
        "            if test_acc > best_nn_acc_sm:\n",
        "                best_nn_acc_sm = test_acc\n",
        "                best_nn_sm = model\n",
        "                best_hist_sm = hist\n",
        "                best_params_nn_sm = {\"layers\": layers, \"lr\": lr, \"dropout\": dr}\n",
        "\n",
        "print(\"Best NN (SMOTE) params:\", best_params_nn_sm)\n",
        "print(\"Best NN (SMOTE) test accuracy:\", best_nn_acc_sm)\n",
        "print(\"Time elapsed (s):\", round(time.time()-t0,1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3yseHkXFwkQ",
        "outputId": "1ca01816-04d8-430c-f93f-164cff08a208"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3.2.1: Evaluate NN (With SMOTE)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "U_41xysBIM1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba_nn_sm = best_nn_sm.predict(X_test_preprocessed).ravel()\n",
        "y_pred_nn_sm = (y_proba_nn_sm > 0.5).astype(int)\n",
        "\n",
        "y_pred_proba_train_nn_sm = best_nn_sm.predict(X_train_sm).ravel()\n",
        "y_pred_train_nn_sm = (y_pred_proba_train_nn_sm > 0.5).astype(int)\n",
        "\n",
        "print_eval_full(\"Decision Tree (with SMOTE)\",\n",
        "                y_train_sm, y_pred_train_nn_sm, y_pred_proba_train_nn_sm,\n",
        "                y_test, y_pred_nn_sm, y_proba_nn_sm)"
      ],
      "metadata": {
        "id": "57fCJA4lHbhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3.2.1: Training Curves for NN**"
      ],
      "metadata": {
        "id": "p9YAGz7xIYVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(best_history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(best_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('NN Training Accuracy (SMOTE)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Acuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(best_history.history['loss'], label='Training Loss')\n",
        "plt.plot(best_history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('NN Training Loss (SMOTE)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cAR38acKIWOt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}